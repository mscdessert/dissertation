\documentclass[12pt,a4paper]{report}

\usepackage{url}
\usepackage{epcc}
\usepackage{graphics}


\begin{document}

\pagenumbering{roman}

\title{Exploiting Private and Hybrid Clouds for Data-intensive Computing}
\author{Roshan.~T.~M}
\date{\today}

\makeEPCCtitle

\thispagestyle{empty}

\vspace{12cm}

\begin{center}

\large{MSc in High Performance Computing}

\large{The University of Edinburgh}

\large{Year of Presentation: 2012}

\end{center}

\newpage

\begin{abstract}

With the advent of the internet, social networks and
the ever increasing number of mobile devices which
help people stay networked, there has been an out burst of unstructured data.
[Amount of data produced] [Reference for unstructured]. There is a potential 
gold mine of information hidden within the data originating from social networks
like Twitter[], Facebook [], Google Plus[] etc. Social networks contain data 
about products people use, what they like and also about their expectations
about a product. A business management team 
can make valuable decisions if necessary information can be extracted from this data
and be made availale to them.
The challenge lies in extracting useful information within a reasonable 
amount of time. The key to extracting useful information with a reasonable
time-frame is distributed data processing.

This work investigates the usage of an open source cloud management toolkit,
OpenNebula [] , along with an open source distributed data processing framework,
Apache Hadoop [] to solve this problem. The benefits of using such an approach
along with the issues it raises are explained. OpenNebula is used to
create a private cloud which provides the infrastructure to build an Apache Hadoop cluster.
 
The distributed data processing framework is benchmarked for its performance
in the private cloud platform. The input/ouput throughput of the framework and
the performance of a word-count application obtained
with varying number of nodes in the cluster is measured and reported. 

Two applications that perform social media analytics are developed. The first
application extracts all the tweets\footnote{The message updated in the twitter is called a tweet}
[] related to London 2012 Olympics
from Twitter and measure the tone of the tweets which provides 
information on how people feel about the Olympics. The second application
extracts all the public tweets and then provides a summary of tweet source,
ie the device people use for tweeting. This information can be used by businesses
to identify potential marketing medium for a product or an event.

OpenNebula and Apache Hadoop proved to be one of pausible solutions to solving
data-intensive problems, however depending on the work-load of the application,
there are factors that limit performance because of Apache Hadoop cluster nodes having to share the physical
resources like hard-disk IO and the network bandwidth.


\end{abstract}

\pagenumbering{roman}

\tableofcontents
\listoftables
\listoffigures

\begin{titlepage}
\vspace*{2in}
% an acknowledgements section is completely optional but if you decide
% not to include it you should still include an empty {titlepage}
% environment as this initialises things like section and page numbering.
\section*{Acknowledgements}

This template is a slightly modified version of the one developed by
Prof. Charles Duncan for MSc students in the Dept. of Meteorology. His
acknowledgement follows:

{\em This template has been produced with help from many former students who
have shown different ways of doing things. Please make suggestions for
further improvements.}

\end{titlepage}

\pagenumbering{arabic}

\chapter{Introduction}

Information is wealth. This statement holds true in all aspects of
life. In the world of commerce, useful information about a product 
can help the management team make strategic business decisions
to improve the product and thus the customer experience. Security establishments
can take corrective measures or respond quickly to a possible security threat
if they have sufficient information before an intrusion. In both the cases
it is very important that the information is made available at the right time. 

\chapter{Background}

\section{Big-data}

% Origin of the term Big-data
% ---------------------------

With more than 2.5 quintillion bytes of data \cite{website:ibm-bigdata} being produced
every day, every now and then we run into a problem of not having 
sufficient computing resources to process and extract useful information
within our required time-frame.

The term big-data is time specific and broadly
big data can be defined at any point in time as ``data whose size forces us
 to look beyond the tried-and-true methods
that are prevalent at that time.`` In the early 1980s, it was a dataset that was
 so large that a robotic ``tape monkey'' was required to swap thousands of tapes in and out.
In the 1990s, perhaps, it was any data that transcended the bounds of Microsoft
Excel and a desktop PC, requiring serious software on Unix workstations to
analyze. Nowadays, it may mean data 
that is too large to be placed in a relational database and analyzed with the
help of a desktop statistics/visualization package-data, perhaps, whose
analysis requires massively parallel software running on tens, hundreds, or
even thousands of servers \cite{jacobs2009pathologies}.

The term big-data is more relevant today because 90\% \cite{website:ibm-bigdata}
of the data in the world today has been created in the last two years and 
hence big-data is getting a lot of hype from business publications and IT blogs
and the mainstream media. 

The shelf-life of the data is short so we should be able to process the data cost effectively,
before its shelf-life expires.


Data sources

There is data being generated from everywhere: sensors used to gather geophysical information, 
posts to social media sites, pictures and videos posted on blogs, transaction
records from e-commerce sites, and mobile GPS signals to name a few.

According to IBM, the challenges of big-data spans three dimensions: Volume,
Velocity and Variety \cite{website:ibm-bigdata}.

\begin{itemize}
  


\item \emph{Volume} \\
Enterprises are awash with ever-growing data of all types,
easily amassing terabytese-ven petabytes-of information.

Enterprises and even small businesses have access to data sources that
produce terabytes and petabytes of information. The volume of data 
gives rise to the challenge of processing the data before the shelf-life
of the data expires. 

Turn 12 terabytes of Tweets created each day into improved product sentiment analysis

Convert 350 billion annual meter readings to better predict power consumption


\item \emph{Velocity} \\
Sometimes 2 minutes is too late. For time-sensitive processes such as catching fraud,
big data must be used as it streams into your enterprise in order to maximize its value.

Scrutinize 5 million trade events created each day to identify potential fraud

Analyze 500 million daily call detail records in real-time to predict customer churn faster


\item \emph{Variety} \\
Big data is any type of data - structured and unstructured data such as text,
sensor data, audio, video, click streams, log files and more. New insights are found when
analyzing these data types together.

Monitor 100's of live video feeds from surveillance cameras to target points of interest
Exploit the 80\% data growth in images, video and documents to improve customer satisfaction

\end{itemize}

%In any case, as analyses of ever-larger datasets become routine, the definition will continue to shift, but one
%thing will remain constant: success at the leading edge will be achieved by
%those developers who can look past the standard, off-the-shelf techniques
%and understand the true nature of the
%hardware resources and the full panoply of algorithms that are available to them



\cite{website:ibm-bigdata} 

{\bf Is Big-data refered only to unstructured data ?}


{\bf Statistics about the amount of data being generated from Social media}

Big-data is commonly refered to data-sets which cannot be processed on a
desktop workstation with in a resonable time-frame.

\section{Distributed processing}

As early as 1978, the idea of using multiple autonomous computers to solve a problem were being
moulded. With the  advent  of  microcomputers and computer networks, distributed processing
became technically and economically feasible \cite{chu1978distributed}

A distributed system consists of multiple autonomous computers that communicate through a computer network.
A problem is divided into many tasks, each of which is solved by one or more computers.

There are different models of distributed computing like:

\subsection{Grid computing}

In a grid computing model the computers are more loosely coupled, heterogeneous, and are geographically dispersed.
The fundamental strategy of grid computing is to use decentralized control to
manage its resources.\cite{foster2002grid} A grid employs standard, open, general-purpose protocols and interfaces to enable interoperability between its heterogeneous infrastructure. 

Grig computing is used to solve a variety of computational problems. The famous SETI@home project 
analyses radio signals from  Arecibo radio telescope\footnote{A radio telescope in the city of 
Arecibo in Puerto Rico operated by Stanford Research Institute and National Science Foundation},
searching for signs of extra terrestrial intelligence.

Grid platform like the Berkeley Open Infrastructure for Network Computing
(BOINC) now allow applications to use GPUs as well.\cite{website:boinc} 

\subsection{Cluster computing}

A cluster is a type of parallel or distributed computer system, which consists of a collection of
inter-connected stand-alone computers working together as a single integrated computing
resource\cite{pfister1998cluster}. The computers in a cluster a generally connected 
by an high performance interconnect. A cluster has a centralized management.

Computer clustering relies on a centralized management approach which makes the nodes available as orchestrated shared servers. It is distinct from other approaches such as peer to peer or grid computing which also use many nodes, but with a far more distributed nature.[3]

%\cite{andrews2002foundations}


\section{Virtualization}

% What is virtualization?
% ----------------------

Virtualization technologies are 
used to subdivide the ample resources of a modern computer and present the illusion of many
smaller virtual machines(VMs)\cite{barham2003xen}.
Virtualization has been applied to operating systems both commercially and in research for nearly forty years.
IBM VM/370 first made use of virtualization to allow binary support for legacy code \cite{gum1983virt}.
Advances is virtualization technologies has enabled computing models like the cloud computing which uses
the virtual machine technology that partition a machine to support the concurrent
execution of multiple operating systems.

% Types of virtualization?
% ----------------------
There are mainly two types of virtualization: \cite{matthews2007quantifying}

\begin{itemize}

 \item \emph{Full virtualization} \\
  A full virtualization technology allows an unmodified operating system binaries to run as 
guests \footnote{The operating system running inside of a VM is traditionally referred to as the guest OS} 
  in a virtual machine. The interface provided by the  virtualization system is the same as the actual physical
 hardware. An example of full virtualization solution is VMware Workstation\cite{website:vmware} and KVM \cite{website:kvm}

 \item \emph{Paravirtualization} \\
  In paravirtualization, specific changes are made in the interface provided to a virtual machine to avoid some features that are difficult or expensive to virtualize. This requires that modifications be made in the operating system to deal with this modified interface. The Xen hypervisor \cite{website:xen} uses paravirtualization.
\end{itemize}


% What are the different V technologies available?
% ------------------------------------------------


% Virtual Machine Monitors?
A hypervisor or virtual machine monitor(VMM) is the software component that allows multiples operating system to run
concurrently on the same hardware. VMMs virtualize all hardware resources, allowing multiple virtual machines to transparently multiplex the resources of the physical machine; this includes memory translation and I/O mapping.

%Hardware assisted virtualization
Hardware assisted virtualization is becoming common and is used to get better efficency when doing virtualization. Hardware vendors are rapidly embracing virtualization and developing new features
to simplify virtualization techniques. First generation enhancements include Intel
Virtualization Technology (VT-x) and AMD's AMD-V which both target privileged 
instructions with a new CPU execution mode feature that allows the VMM to run in a privileged mode and thus  sensitive calls are set to automatically trap to the hypervisor, removing the need for either binary translation or paravirtualization \cite{website:vmware}


% Benefits of virtualization.
% ---------------------------

Virtualization provides enables better IT infrastructure management with benefits like \cite{website:intel-VT}:

\begin{itemize}
  
\item \emph{Server Consolidation} \\
Several sever applications that require different operating system and technical specification 
environment can now be deployed on a single physical machine. Though the server hosting the guest 
OSs will have more memory, CPU and other hardware, it will use little or no more power and occupy 
the same physical space and thus reduce real-estate and utility expenditures.

\item \emph{Testing and development} \\
The use of VMs provides the ability to deploy application is an isolated manner and
in a controlled environment. System recovery from a crash is much faster as it uses a virtual
image.

\item \emph{Dynamic Load Balancing and Disaster Recovery} \\
Virtualization provides the ability for VMs that are over utilizing  the resources of a server to be moved to 
another server that is underutilized. Such load balancing improves utilization of server resources.

\item \emph{Improved System Reliability and Security} \\
Virtualization of the hardware prevents system crashes that occur due to memory corruption caused by 
software device drivers. Virtualization provides methods to better control system devices by defining the
architecture for DMA and interrupt remapping to ensure improved isolation of I/O resources for greater
reliability, security, and availability.
\end{itemize}

\section{Cloud Computing}

Cloud computing is a model where a pool of resources (hardware/software) are
managed to deliver them as virtualised resources on-demand. Virtualization is the key
to Cloud computing.


Cloud Computing was a concept first publicly announced by John McCarthy in 1961 
(in a speech given to celebrate MIT's centennial). He saw a future in which computing 
power and even specific applications could be sold through the utility 
business model like water or electricity \cite{garfinkel1999architects}.
This concept and  supporting technologies evolved over the period and has taken us 
to the reality where computing resources are served as a utility. 
Technologies relating to CPU Power, network bandwidth, memory capacity 
and connectivity have improved dramatically in this period enabling Cloud Computing as 
a business model.

Cloud Computing is of interest among IT and network technologists
and academics, as well as CIOs and business leaders, due to its potential to 
reduce cost and risk, increase revenue, and enhance total customer experience.\cite{weinman2011future}
It has been difficult to give a concise definition for Cloud Computing 
and related terms. According to the U.S. National Institute of Standards 
and Technology (NIST), \cite{mell2009nist}  "Cloud computing is a model for enabling convenient,
on-demand network access to a shared pool of configurable computing resources
(e.g., networks, servers, storage, applications, and services) that can 
be rapidly provisioned and released with minimal management 
effort or service provider interaction." 

virtual machine monitor (VMM) which enables applications such as server consolidation [42, 8], co-located hosting
facilities [14], distributed web services [43], secure computing platforms [12, 16] and application
mobility

The essential characteristics are \cite{mell2009nist} :

\begin{itemize}
  \item \emph{On-demand self services} \\ Provision resources without human
     interaction with service provider
  \item \emph{Broad network access} \\Resources available over the network
  \item \emph{Resource pooling} \\ Resources are pooled to serve multiple 
        consumers using a multi-tenancy model
  \item \emph{Rapid elasticity} \\Ability to increase or decrease resource consumption
  \item \emph{Measured service} \\Resource usage can be monitored, controlled and reported.
\end{itemize}

There are three common ways to deploy the Cloud:

\begin{enumerate}
 \item \emph{Private Cloud} \\The Cloud resources are available only 
to a single organization and it is owned and managed by 
that organization or a third party. Typically the infrastructure is in premises of the 
organization.
 \item \emph{Public Cloud} \\The Cloud resources are provisioned for use by the general 
public and the infrastructure is owned, managed and operated by the Cloud provider.
  \item \emph{Hybrid Cloud} \\The Cloud infrastructure is a combination of 
both the private Cloud and the public Cloud with support for data and application 
portability.
\end{enumerate}


\section{Apache Hadoop}

Apache Hadoop \cite{website:apache-hadoop} is a top-level Apache project that includes open source implementations of a distributed file system \cite{website:apache-hdfs} and MapReduce that was inspired by Google's GFS
\cite{ghemawat2003google} and MapReduce \cite{dean2008mapreduce} projects.

The open source Apache Hadoop software library is a framework that 
allows the distributed processing of large data sets across computer cluster using the MapReduce programming model \cite{website:apache-hadoop}. It has a distributed file system, the Hadoop Distributed File System, that provides 
high-throughput access to application data.

Facebook uses the Apache Hadoop platform to run it messages application \footnote {Facebook Messages gives every 
Facebook user a facebook.com email address, integrates the display of all e-mail, SMS and chat messages between a pair or group of users, has strong controls over who users receive messages from, and is the foundation of a Social Inbox\cite{borthakur2011apache}.}. Hadoop was chosen because it met the application's reqirements for consistency, availability, partition tolerance, data model and scalability \cite{borthakur2011apache}.


The Hadoop ecosystem also includes projects like:\cite{website:apache-hadoop}

\begin{itemize}
\item \emph{Avro:} A data serialization system.
\item \emph{Cassandra:} A scalable multi-master database with no single points of failure.
\item \emph{Chukwa:} A data collection system for managing large distributed systems.
\item \emph{HBase:} A scalable, distributed database that supports structured data storage for large tables. This was inspired by Google's BigTable.
\item \emph{Hive:} A data warehouse infrastructure built on top of Hadoop that provides data summarization and ad hoc querying.
\item \emph{Mahout:} A Scalable machine learning and data mining library.
\item \emph{Pig:} A high-level data-flow language and execution framework for parallel computation.
\item \emph{ZooKeeper:} A high-performance coordination service for distributed applications.
\end{itemize}

\subsection{Apache MapReduce}

Google introduced the MapReduce to crunch its huge data sets. 
It is a programming model and an associated implementation for processing 
and generating large data sets. Users specify a map function that processes a 
key/value pair to generate a set of intermediate key/value pairs, and a reduce 
function that merges all intermediate values associated with the same 
intermediate key. MapReduce runs on a large cluster of commodity machines 
and is highly scalable \cite{dean2008mapreduce}. 

{\bf Google produce MapReduce to process data generated from their web-crawler}

\subsection{Apache HDFS}

\section{OpenNebula}

OpenNebula is an open source toolkit for building and managing virtualized
cloud infrastructure \cite{website:one}. The OpenNebula tookit provides a flexible
solution by providing interoperability with multiple hardware and software combinations.
It supports the Xen, KVM and VMWare hypervisors and provides both command line
and graphical interfaces to its features.


\section{Amazon Web Services (AWS)}

\subsection{Amazon EC2}

\chapter{Cloud Infrastructure Design}

\section{Cloud hardware and software stack}
\subsection{EPCC machines}
\subsection{Intel VT-x}

Intel VT-x is Intel's implementation of hardware-assisted x86 virtualization.
Intel Virtualization Technology provides the hardware support for efficient processor virtualization.
This hardware technology for virtualization when combined with software based virtualization provides 
enhanced overall system utilization.\cite{website:intel-VT}

Hardware-based virtualization technologies like the Intel Virtualization Technology
improves the robustness of software-based virtualization solutions by accelerating
fundamental functions of the virtualised platform like:\cite{neiger2006intel}

\begin{itemize}
 \item Speeding up the transfer of platform control between the guest operating systems (OSs) and the virtual machine manager (VMM)/hypervisor like KVM
 \item Enabling the VMM to uniquely assign I/O devices to guest OSs
 \item Optimizing the network for virtualization with adapter-based acceleration
\end{itemize}





\subsection{Ubuntu 11.10}
\subsection{KVM}
\subsection{QEMU}

QEMU is a fast processor emulator that uses a portable dynamic translator. It supports two operating
modes: user space only, and full system emulation. In the earlier mode, QEMU can launch Linux processes
compiled for one CPU on another CPU, or for cross-compilation and cross-debugging. In the later mode, it
can emulate a full system that includes a processor and several peripheral devices. It supports emulation of a
number of processor architectures that includes x86, ARM, PowerPC, and Sparc, unlike Bochs that is closed
tied with the x86 architecture. Like Crusoe, it uses a dynamic translation to native code for reasonable speed.
In addition, its features include support for self-modifying code and precise exceptions. Both full software
MMU and simulation through mmap() system call on the host are supported. \cite{chiueh2005survey}

During dynamic translation, it converts a piece of encountered code to the host instruction set. The
basic idea is to split every x86 instruction (e.g.) into fewer simpler instructions. Each simple instruction
is implemented by a piece of C code and then a compile time tool takes the corresponding object ﬁle to a
dynamic code generator which concatenates the simple instructions to build a function. More such tricks
enable QEMU to be relatively easily portable and simple while achieving high performances. Like Crusoe,
it also uses a 16MB translation cache and ﬂushes to empty when it gets ﬁlled. It uses a basic block as a
translation unit. Self-modifying code is a special challenge in x86 emulation because no instruction cache

\subsection{Libvirt}

\section{Cloud monitoring}
\subsection{OpenNebula Sunstone GUI}
\subsection{Ganglia}

\chapter{Hadoop Cluster Design}

\section{Hadoop software stack}
\subsection{Cloudera Hadoop distribution}
\section{Hadoop cluster on Private cloud}
\section{Hadoop cluster on Hybrid cloud}
\section{Running a MapReduce program}
\section{Benchmarking}


%\chapter{Experimental design}
\chapter{Twitter Data Analysis}

\section{Streaming API - Gathering the Tweets}

\subsection{Tweets included in Streaming API}

FROM: https://dev.twitter.com/docs/streaming-api/concepts
[
Only non-protected public accounts can create public statuses. 
Statuses, including replies and mentions, created by a public 
account are candidates for inclusion in the Streaming API. Statuses 
created by protected accounts and all direct messages are non-public 
and are currently not available via the Streaming API, but are 
available on User Streams and Site Streams.
]

\subsection{Twitter Policies}

May also want to include more details from

Twitter Search Rules and Restrictions; link same as below

FROM: http://support.twitter.com//forums/10713/entries/42646
[
In order to keep your search results relevant, Twitter filters search 
results for quality. Our search results will not include suspended accounts, 
or accounts that may jeopardize search quality. Material that degrades 
search relevancy or creates a bad search experience for 
people using Twitter may be permanently removed.
]
\subsection{Result Quality}

FROM: https://dev.twitter.com/docs/streaming-api/concepts
[
Both the Streaming API and the Search API filter, and on some end-points,
discard, statuses created by a small proportion of accounts based upon status
quality metrics. For example, frequent and repetitious status updates may,
in some instances, and in combination with other metrics, result in a
different status quality score for a given account. Results that are not selected
by user id, for example: samples and keyword track, are filtered by this
status quality metric. Results that are selected by user id, currently only
results from the follow predicate, are unfiltered and allow all
matching statuses to pass. If an expected user's statuses are not present in a
non-follow-predicate stream type, manually cross-check the user against Search
results. If the user's statuses are also not returned in Search, you can assume
that the user's statuses will not be returned by non-follow-predicated streams.
]

\section{Processing the Tweets}

\chapter{Results and Analysis}


\chapter{Conclusions}

This is the place to put your conclusions about your work. You can
split it into different sections if appropriate. You may want to include
a section of future work which could be carried out to continue your
research.

\appendix
% the appendix command just changes heading styles for appendices.

\chapter{OpenNebula Configuration Details}

Appendices should contain all the material which is considered too
detailed to be included in the main bod but which is, nevertheless,
important enough to be included in the thesis. 

\chapter{Hadoop Cluster Configuration Details}

Some  people include in their \cite{fox2009above}thesis a lot of detail, particularly
computer code, which no-one will ever read. You should be careful that
anything like this you include \cite{website:aws} should contain some element
 of \cite{weinman2011future}uniqueness which justifies its inclusion.



\bibliographystyle{plain}
\bibliography{msc-biblio}
\end{document}


